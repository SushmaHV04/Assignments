{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f014e3-c73c-4d7c-b3f9-140183d3606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0673c66b-2847-4115-987d-b23d48a9453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the website.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.flipkart.com/search?q=iphone'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected to the website.\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9148061-993e-435d-b1a3-ddb182bdd0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Product Name', 'Price', 'Rating', 'Reviews'], dtype='object')\n",
      "[{'Product Name': 'iPhone 14 Pro Max', 'Price': '$1099', 'Rating': 4.8, 'Reviews': '15,000 reviews'}, {'Product Name': 'iPhone 14 Pro', 'Price': '$999', 'Rating': 4.7, 'Reviews': '12,000 reviews'}, {'Product Name': 'iPhone 14', 'Price': '$799', 'Rating': 4.6, 'Reviews': '20,000 reviews'}, {'Product Name': 'iPhone 13 Pro Max', 'Price': '$999', 'Rating': 4.8, 'Reviews': '18,000 reviews'}, {'Product Name': 'iPhone 13', 'Price': '$699', 'Rating': 4.7, 'Reviews': '25,000 reviews'}]\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "data = []\n",
    "for _, row in df.iterrows():\n",
    "    product_data = {\n",
    "        'Product Name': row['Product Name'],\n",
    "        'Price': row['Price'],\n",
    "        'Rating': row['Rating'],\n",
    "        'Reviews': row['Reviews']\n",
    "    }\n",
    "    data.append(product_data)\n",
    "\n",
    "print(data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc3043a-bfc2-4de2-a742-c2216d65b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extraction Completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sample_iphone_data.csv')\n",
    "\n",
    "data = []\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        product_name = row.get('Product Name', 'N/A')\n",
    "        price = row.get('Price', 'N/A')\n",
    "        rating = row.get('Rating', 'N/A')\n",
    "        reviews = row.get('Reviews', 'N/A')\n",
    "\n",
    "        if pd.isna(price) or price == 'N/A':\n",
    "            print(f\"Warning: Missing price for {product_name}\")\n",
    "\n",
    "        if pd.isna(rating) or rating == 'N/A':\n",
    "            print(f\"Warning: Missing rating for {product_name}\")\n",
    "\n",
    "        if pd.isna(reviews) or reviews == 'N/A':\n",
    "            print(f\"Warning: Missing reviews for {product_name}\")\n",
    "\n",
    "        product_data = {\n",
    "            'Product Name': product_name,\n",
    "            'Price': price,\n",
    "            'Rating': rating,\n",
    "            'Reviews': reviews\n",
    "        }\n",
    "        data.append(product_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row}: {e}\")\n",
    "\n",
    "print(\"Data Extraction Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ebde1e-0bd0-42fb-be26-56e3b82dc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to iphones_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "iphone_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "iphone_df.to_csv('iphones_data.csv', index=False)\n",
    "\n",
    "print(\"Data successfully saved to iphones_data.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdff6bb6-139e-4e7d-b8cd-cc32ef43b141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve page 1\n",
      "Failed to retrieve page 2\n",
      "Failed to retrieve page 3\n",
      "Scraping completed and data saved.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_iphone_listings(base_url, num_pages=5):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        product_list = soup.find_all(\"div\", class_=\"product-class\")  # Update with correct class\n",
    "\n",
    "        for product in product_list:\n",
    "            try:\n",
    "                name = product.find(\"a\", class_=\"product-title-class\").text.strip()\n",
    "                price = product.find(\"div\", class_=\"price-class\").text.strip()\n",
    "                rating = product.find(\"div\", class_=\"rating-class\").text.strip() if product.find(\"div\", class_=\"rating-class\") else \"N/A\"\n",
    "                reviews = product.find(\"span\", class_=\"reviews-class\").text.strip() if product.find(\"span\", class_=\"reviews-class\") else \"N/A\"\n",
    "                \n",
    "                all_data.append({\"Product Name\": name, \"Price\": price, \"Rating\": rating, \"Reviews\": reviews})\n",
    "            \n",
    "            except AttributeError:\n",
    "                print(f\"Skipping a product due to missing data.\")\n",
    "\n",
    "        time.sleep(2)  \n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "base_url = \"https://www.flipkart.com/search?q=iphone\"  \n",
    "iphone_df = scrape_iphone_listings(base_url, num_pages=3)\n",
    "\n",
    "\n",
    "iphone_df.to_csv(\"scraped_iphones_data.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9d708-ed53-4c98-b38b-2d1e09bf837b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
